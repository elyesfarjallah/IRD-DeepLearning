{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.modular_data_extraction import create_data_packages\n",
    "from data_pipeline.np_dataset import NpDataset\n",
    "import input_mapping.models_torch as models_torch\n",
    "from data_pipeline.image_transforms import get_transforms\n",
    "from data_pipeline.ukb_data_extractor import UkbDataExtractor\n",
    "import data_pipeline.data_processing_utils as dpu\n",
    "\n",
    "from data_pipeline.data_package import DataPackage\n",
    "from data_pipeline.odir_5k_data_extractor import ODIR5KDataExtractor\n",
    "from data_pipeline.rfmid_data_extractor import RFMiDDataExtractor\n",
    "from data_pipeline.rfmid2_data_extractor import RFMiD2DataExtractor\n",
    "from data_pipeline.ukb_data_extractor import UkbDataExtractor\n",
    "from data_pipeline.rips_data_extractor import RIPSDataExtractor\n",
    "from data_pipeline.ses_data_extractor import SESDataExtractor\n",
    "from data_pipeline.one_thousand_images_data_extractor import OneThousandImagesDataExtractor\n",
    "\n",
    "from data_pipeline.data_processing_utils import standardize_labels\n",
    "from data_pipeline.data_processing_utils import create_one_hot_encoder\n",
    "import data_pipeline.data_processing_utils as dpu\n",
    "import data_pipeline.data_splitting_utils as dsu\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from pydicom import dcmread\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "from convert_user_inputs import convert_user_input, create_datasets\n",
    "from ai_backend.loggers.model_logger import is_min\n",
    "from uuid import uuid4\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "from ai_backend.evaluators.metrics.multi_label_metrics import  multi_label_f_beta, multi_label_confusion_matrix, multi_label_accuracy, multi_label_precision, multi_label_recal\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = 'resnet18'\n",
    "transform_type = 'ben'\n",
    "\n",
    "dataset_name = '2024-02-23-13-31-39'\n",
    "dataset_path = f'datasets/{dataset_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_encode = np.array([\"Age-related Macular Degeneration\", \"Best Disease\", \"Bietti crystalline dystrophy\",\n",
    "                              \"cataract\", \"Cone Dystrophie or Cone-rod Dystrophie\", \"Diabetic Retinopathy\",\n",
    "                              \"glaucoma\", \"Maculopathy\", \"Myopia\", \"Normal\", \"Retinitis Pigmentosa\", \"Stargardt Disease\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukb_database_path = 'databases/ird_dataset/IRD-Dataset-Complete-03-anonymized.xlsx'\n",
    "ukb_data_path ='databases/ird_dataset/export_heyex_original_dataset_03/DICOM'\n",
    "ukb_extractor = UkbDataExtractor(database_path=ukb_data_path, label_path=ukb_database_path)\n",
    "\n",
    "odir5k_data_extractor = ODIR5KDataExtractor(database_path='databases/ODIR-5K/full_df.csv', database_test_images_path='databases/ODIR-5K/Testing Images',\n",
    "                                                database_train_images_path='databases/ODIR-5K/Training Images')\n",
    "\n",
    "rfmid_train_data_extractor = RFMiDDataExtractor(database_path='databases/RFMiD/Training_Set/RFMiD_Training_Labels.csv',\n",
    "                                            data_path='databases/RFMiD/Training_Set/Training', file_format='png')\n",
    "\n",
    "rfmid_validation_datae_xtractor = RFMiDDataExtractor(database_path='databases/RFMiD/Evaluation_Set/RFMiD_Validation_Labels.csv',\n",
    "                                            data_path='databases/RFMiD/Evaluation_Set/Validation', file_format='png')\n",
    "\n",
    "rfmid_test_data_extractor = RFMiDDataExtractor(database_path='databases/RFMiD/Test_Set/RFMiD_Testing_Labels.csv',\n",
    "                                                data_path='databases/RFMiD/Test_Set/Test', file_format='png')\n",
    "\n",
    "rfmid2_train_data_extractor = RFMiD2DataExtractor(database_path='databases/RFMiD2_0/Training_set/RFMiD_2_Training_labels.csv',\n",
    "                                                    data_path='databases/RFMiD2_0/Training_set')\n",
    "rfmid2_validation_data_extractor = RFMiD2DataExtractor(database_path='databases/RFMiD2_0/Validation_set/RFMiD_2_Validation_labels.csv',\n",
    "                                                            data_path='databases/RFMiD2_0/Validation_set')\n",
    "\n",
    "rfmid2_test_data_extractor = RFMiD2DataExtractor(database_path='databases/RFMiD2_0/Test_set/RFMiD_2_Testing_labels.csv',\n",
    "                                                        data_path='databases/RFMiD2_0/Test_set')\n",
    "\n",
    "\n",
    "one_thousand_images_data_extractor = OneThousandImagesDataExtractor(database_path='databases/1000images/')\n",
    "\n",
    "rips_data_extractor = RIPSDataExtractor(database_path='databases/RIPS/Original')\n",
    "\n",
    "ses_data_extractor = SESDataExtractor(database_path='databases/SES/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the data extraction list\n",
    "default_data_extractors = [odir5k_data_extractor, rfmid_train_data_extractor, rfmid_validation_datae_xtractor, rfmid_test_data_extractor,\n",
    "                    rfmid2_train_data_extractor, rfmid2_validation_data_extractor, rfmid2_test_data_extractor,\n",
    "                    one_thousand_images_data_extractor, rips_data_extractor, ses_data_extractor]\n",
    "\n",
    "dicom_data_extractors = [ukb_extractor]\n",
    "\n",
    "data_extractors = default_data_extractors + dicom_data_extractors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the data\n",
    "for data_extractor in data_extractors:\n",
    "    data_extractor.extract()\n",
    "\n",
    "#standardize the data\n",
    "#get the labels of the data\n",
    "datasets_labels = []\n",
    "for data_extractor in data_extractors:\n",
    "    datasets_labels.append(data_extractor.get_labels())\n",
    "#flatten\n",
    "labels = []\n",
    "for dataset_labels in datasets_labels:\n",
    "    labels.extend(dataset_labels)\n",
    "#concatenate the labels\n",
    "labels = np.concatenate(labels)\n",
    "#drop the None values\n",
    "labels = labels[labels != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_summarize_set = set(RFMiD2DataExtractor.abbreviation_map.values())\n",
    "ukb_label_mapping_dict = {'Morbus Best': 'Best Disease', 'Morbus Stargardt': 'Stargardt Disease', 'Retinitis pigmentosa': 'Retinitis Pigmentosa'}\n",
    "label_standertizer = standardize_labels(labels = labels, not_summarize_set=not_summarize_set)\n",
    "label_standertizer.update(ukb_label_mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value count the labels\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "#get the median label count\n",
    "median_label_count = label_counts.median()\n",
    "label_instance_limit = int((max(label_counts) - median_label_count) // 4)\n",
    "#balance the labels\n",
    "#find the over represented labels\n",
    "for labels,extractor in zip(datasets_labels, data_extractors):\n",
    "    #find the over represented labels\n",
    "    #replace none with empty string\n",
    "    over_represented_labels_idxs, _, _ = dpu.find_over_represented_samples(file_paths=extractor.get_file_paths(), labels=labels,\n",
    "                                                                            max_samples_per_class=label_instance_limit)\n",
    "    #remove the over represented labels\n",
    "    #conver the indexes to a boolean array\n",
    "    over_represented_labels_series = np.isin(np.arange(len(labels)), over_represented_labels_idxs)\n",
    "    extractor.extracted_data = extractor.extracted_data[~over_represented_labels_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data stratified by the labels\n",
    "train_portion = 0.7\n",
    "val_portion = 0.1\n",
    "test_portion = 0.2\n",
    "split_portions = [train_portion, val_portion, test_portion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = []\n",
    "for data_extractor in data_extractors:\n",
    "    splits.extend(data_extractor.split_extracted_data(split_portions = split_portions, stratify=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_strip = lambda x: x.strip() if isinstance(x, str) else x\n",
    "label_translation = lambda x: label_standertizer.get(x, x)\n",
    "for i, split in enumerate(splits):\n",
    "    #strip trailing and leading whitespaces\n",
    "    #split.labels = np.vectorize(lambda_strip)(split.labels)\n",
    "    split.labels = np.vectorize(label_translation)(split.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the labels\n",
    "all_labels = []\n",
    "for split in splits:\n",
    "    all_labels.extend(split.labels)\n",
    "#create set of all labels\n",
    "all_labels = np.concatenate(all_labels)\n",
    "#filter out the None values\n",
    "all_labels = all_labels[all_labels != None]\n",
    "all_labels = np.unique(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = dpu.create_one_hot_encoder(unique_labels=labels_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    split.labels = dpu.encode_multistring_labels(split.labels, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2977\n"
     ]
    }
   ],
   "source": [
    "print(len(splits[0].get_labels()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODIR-5K\n",
      "n filtered 1641 out of 2977\n",
      "ODIR-5K\n",
      "n filtered 232 out of 425\n",
      "ODIR-5K\n",
      "n filtered 467 out of 850\n",
      "RFMiD\n",
      "n filtered 671 out of 1341\n",
      "RFMiD\n",
      "n filtered 103 out of 200\n",
      "RFMiD\n",
      "n filtered 189 out of 379\n",
      "RFMiD\n",
      "n filtered 215 out of 444\n",
      "RFMiD\n",
      "n filtered 34 out of 66\n",
      "RFMiD\n",
      "n filtered 64 out of 130\n",
      "RFMiD\n",
      "n filtered 225 out of 448\n",
      "RFMiD\n",
      "n filtered 33 out of 64\n",
      "RFMiD\n",
      "n filtered 63 out of 128\n",
      "RFMiD2\n",
      "n filtered 178 out of 310\n",
      "RFMiD2\n",
      "n filtered 30 out of 50\n",
      "RFMiD2\n",
      "n filtered 57 out of 95\n",
      "RFMiD2\n",
      "n filtered 59 out of 106\n",
      "RFMiD2\n",
      "n filtered 9 out of 15\n",
      "RFMiD2\n",
      "n filtered 21 out of 35\n",
      "RFMiD2\n",
      "n filtered 63 out of 104\n",
      "RFMiD2\n",
      "n filtered 10 out of 16\n",
      "RFMiD2\n",
      "n filtered 16 out of 29\n",
      "1000images\n",
      "n filtered 561 out of 699\n",
      "1000images\n",
      "n filtered 81 out of 100\n",
      "1000images\n",
      "n filtered 162 out of 201\n",
      "RIPS\n",
      "n filtered 0 out of 60\n",
      "RIPS\n",
      "n filtered 0 out of 30\n",
      "RIPS\n",
      "n filtered 0 out of 30\n",
      "SES\n",
      "n filtered 2 out of 87\n",
      "SES\n",
      "n filtered 0 out of 12\n",
      "SES\n",
      "n filtered 0 out of 26\n",
      "UKB\n",
      "n filtered 745 out of 1427\n",
      "UKB\n",
      "n filtered 90 out of 204\n",
      "UKB\n",
      "n filtered 225 out of 407\n"
     ]
    }
   ],
   "source": [
    "#find out which datapoints have a full 0 label\n",
    "for split in splits:\n",
    "    print(split.data_source_name)\n",
    "    labels = split.get_labels()\n",
    "    no_zero_labels = np.sum(labels, axis=1) != 0\n",
    "    #print len false values\n",
    "    print('n filtered', len(no_zero_labels) - np.sum(no_zero_labels), 'out of', len(no_zero_labels))\n",
    "    #throw away the datapoints with no labels\n",
    "    split.labels = labels[no_zero_labels]\n",
    "    split.data = split.data[no_zero_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1336"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(default_data_extractors[0].get_current_split()[0].get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the splits list so that always 3 splits are in a row\n",
    "buffer = []\n",
    "for i in range(0, len(splits), 3):\n",
    "    inner_list = splits[i:i + 3]\n",
    "    buffer.append(inner_list)\n",
    "splits = buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "dataset_path = f'datasets/{time}'\n",
    "train_datas_save_path = f'{dataset_path}/train'\n",
    "val_datas_save_path = f'{dataset_path}/val'\n",
    "test_datas_save_path = f'{dataset_path}/test'\n",
    "save_path_list = [train_datas_save_path, val_datas_save_path, test_datas_save_path]\n",
    "os.makedirs(train_datas_save_path, exist_ok=True)\n",
    "os.makedirs(val_datas_save_path, exist_ok=True)\n",
    "os.makedirs(test_datas_save_path, exist_ok=True)\n",
    "#crearte a dataset configuration\n",
    "dataset_config = {'labels_to_encode': labels_to_encode, 'label_standertizer': label_standertizer}\n",
    "with open(f'{dataset_path}/dataset_config.json', 'w') as f:\n",
    "    json.dump(dataset_config, f)\n",
    "for split in splits:\n",
    "    for package, path in zip(split, save_path_list):\n",
    "        total_path = f'{path}/{package.data_source_name}.json'\n",
    "        #check if the path already exists\n",
    "        if os.path.exists(total_path):\n",
    "            #if it exists, append a uuid to the path\n",
    "            total_path = f'{path}/{package.data_source_name}_{str(uuid4())[:4]}.json'\n",
    "        package.save(f'{total_path}')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ird_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
