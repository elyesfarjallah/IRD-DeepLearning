{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pipeline.np_dataset import NpDataset\n",
    "import input_mapping.models_torch as models_torch\n",
    "from data_pipeline.image_transforms import get_transforms\n",
    "\n",
    "from data_pipeline.data_package import DataPackage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from pydicom import dcmread\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "from ai_backend.loggers.model_logger import is_min\n",
    "from uuid import uuid4\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "from ai_backend.evaluators.metrics.multi_label_metrics import  multi_label_f_beta, multi_label_confusion_matrix, multi_label_accuracy, multi_label_precision, multi_label_recal\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint 1\n",
    "augmentations = transforms.Compose([\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.RandomHorizontalFlip(p=0.6),\n",
    "                transforms.RandomVerticalFlip(p=0.5)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create np datasets for training, validation and testing\n",
    "read_dicom = lambda x: dcmread(x).pixel_array\n",
    "dicom_file_reader = lambda x: Image.fromarray(read_dicom(x)).convert('RGB')\n",
    "default_file_reader = lambda x: Image.open(x).convert('RGB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = 'resnet18'\n",
    "transform_type = 'standard'\n",
    "batch_size = 128\n",
    "lr = 0.00009354747253832916\n",
    "epochs = 30\n",
    "transforms_config = models_torch.model_dict[model_key]['transforms_config']\n",
    "transform = get_transforms(transform_name = transform_type, transforms_config = transforms_config)\n",
    "labels_to_encode = np.array([\"Age-related Macular Degeneration\", \"Best Disease\", \"Bietti crystalline dystrophy\",\n",
    "                              \"cataract\", \"Cone Dystrophie or Cone-rod Dystrophie\", \"Diabetic Retinopathy\",\n",
    "                              \"glaucoma\", \"Maculopathy\", \"Myopia\", \"Normal\", \"Retinitis Pigmentosa\", \"Stargardt Disease\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_package_to_dataset(package, augmentations=None):\n",
    "    file_reader = dicom_file_reader if package.data_source_name == 'UKB' else default_file_reader\n",
    "    dataset = NpDataset(file_paths=package.get_data(), labels=package.get_labels(),\n",
    "                         file_reader=file_reader, transform=transform, augmentation_transform=augmentations)\n",
    "    return dataset\n",
    "\n",
    "def convert_package_list_to_dataset(package_list, augmentations=None):\n",
    "    datasets = []\n",
    "    for package in package_list:\n",
    "        dataset = convert_package_to_dataset(package)\n",
    "        datasets.append(dataset)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the saved directories and load the datapackages\n",
    "dataset_path = 'datasets/2024-05-17_12-13-57'\n",
    "train_packages_path = f'{dataset_path}/train'\n",
    "val_packages_path = f'{dataset_path}/val'\n",
    "test_packages_path = f'{dataset_path}/test'\n",
    "package_paths = [train_packages_path, val_packages_path, test_packages_path]\n",
    "train_packages = []\n",
    "val_packages = []\n",
    "test_packages = []\n",
    "all_packages = [train_packages, val_packages, test_packages]\n",
    "\n",
    "for path, package_list  in zip(package_paths, all_packages):\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        package = DataPackage.load(f'{path}/{file}')\n",
    "        package_list.append(package)\n",
    "#convert to np datasets\n",
    "train_datasets = convert_package_list_to_dataset(train_packages, augmentations=augmentations)\n",
    "val_datasets = convert_package_list_to_dataset(val_packages)\n",
    "test_datasets = convert_package_list_to_dataset(test_packages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in  enumerate(train_datasets):\n",
    "    dataset.balance_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the datasets\n",
    "train_dataset = ConcatDataset(train_datasets)\n",
    "validation_dataset = ConcatDataset(val_datasets)\n",
    "test_dataset = ConcatDataset(test_datasets)\n",
    "\n",
    "#create data loaders\n",
    "num_workers = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = models_torch.get_model(model_name=model_key, num_classes=len(labels_to_encode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add dropout forward hooks to the model\n",
    "for name, module in model.named_modules():\n",
    "    re_pattern = re.compile(r'^layer\\d+$')\n",
    "    if re_pattern.match(name) is not None:\n",
    "        print('Adding forward hook for:', name)\n",
    "        module.register_forward_hook(lambda module, input,\n",
    "                                      output: torch.nn.functional.dropout2d(output, p=0.2, training=module.training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = str(uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "dataset_name = '2024-05-17_12-13-57'\n",
    "best_model_save_folder = f'models/{model_key}/{model_id}'\n",
    "best_model_save_path = f'{best_model_save_folder}/weights.pt'\n",
    "os.makedirs(best_model_save_folder, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#create model configuration\n",
    "model_config = {'model_key': model_key, 'transform_type': transform_type, 'batch_size': batch_size, 'lr': lr, 'epochs': epochs,\n",
    "                'labels_to_encode': labels_to_encode.tolist(), 'model_id': model_id, 'dataset_name': dataset_name}\n",
    "\n",
    "epochs = 30\n",
    "progress_bar = tqdm.tqdm(range(epochs))\n",
    "validation_loss_criterion = nn.BCEWithLogitsLoss()\n",
    "validation_losses = []\n",
    "best_validation_loss = np.inf\n",
    "#move the model to the device\n",
    "model.to(device)\n",
    "#only run training if no model has been saved yet\n",
    "if not os.path.exists(best_model_save_path):\n",
    "    for epoch in progress_bar:\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            validaton_outputs = []\n",
    "            validaton_labels = []\n",
    "            for inputs, labels in validation_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                validaton_outputs.append(outputs)\n",
    "                validaton_labels.append(labels)\n",
    "            #concatenate the outputs and labels\n",
    "            validaton_outputs = torch.cat(validaton_outputs, dim=0)\n",
    "            validaton_labels = torch.cat(validaton_labels, dim=0)\n",
    "            loss_validation = validation_loss_criterion(validaton_outputs, validaton_labels).item()\n",
    "            if is_min(loss_validation, best_validation_loss):\n",
    "                best_validation_loss = loss_validation\n",
    "                torch.save(model.state_dict(), best_model_save_path)\n",
    "                #update the model configuration\n",
    "                model_config['epoch'] = epoch\n",
    "                #best validation loss\n",
    "                model_config['best_validation_loss'] = best_validation_loss\n",
    "                #save the model configuration\n",
    "                with open(f'{best_model_save_folder}/model_config.json', 'w') as f:\n",
    "                    json.dump(model_config, f)\n",
    "            validation_losses.append(loss_validation)\n",
    "            #update the progress bar\n",
    "            progress_bar.set_postfix({'Loss validation': loss_validation, 'best validation loss': min(validation_losses)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best model\n",
    "model.load_state_dict(torch.load(best_model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_save_folder = f'{best_model_save_folder}/evaluations'\n",
    "os.makedirs(evaluations_save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.savefig(f'{evaluations_save_folder}/validation_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the predictions for the model\n",
    "y_true_validation = []\n",
    "y_pred_validation = []\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in validation_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_true_validation.append(labels.detach().clone())\n",
    "        y_pred_validation.append(outputs.detach().clone())\n",
    "y_true_validation = torch.cat(y_true_validation, dim=0).cpu()\n",
    "y_pred_validation = torch.cat(y_pred_validation, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "for data, label in validation_dataset:\n",
    "    #plot the image\n",
    "    if random.random() < 0.5:\n",
    "        plt.imshow(to_pil_image(data))\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best threshold for each label\n",
    "#turn off optuna warnings\n",
    "step_size = 0.01\n",
    "iterations = 1/step_size\n",
    "#set the verbosity to error\n",
    "\n",
    "best_thresholds = []\n",
    "\n",
    "for i in range(len(labels_to_encode)):\n",
    "    best_thresholds.append(0)\n",
    "    best_score = 0\n",
    "    #todo fix the max function\n",
    "    for j in range(int(iterations)):\n",
    "        threshold = j*step_size\n",
    "        f1_score = multi_label_f_beta(y_true_validation, y_pred_validation, beta=1.0, averaging_type=None, threshold=threshold)\n",
    "        if f1_score[i] > best_score:\n",
    "            best_score = f1_score[i]\n",
    "            best_thresholds[i] = threshold\n",
    "    print('Best thresholds found for class', labels_to_encode[i], 'at', best_thresholds[i], 'with a score of', best_score)\n",
    "#save the best thresholds\n",
    "model_config = {\n",
    "    'best_thresholds': best_thresholds\n",
    "}\n",
    "\n",
    "best_thresholds = torch.Tensor(best_thresholds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best thresholds:', best_thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the best thresholds in the model configuration\n",
    "model_config['best_thresholds'] = best_thresholds.tolist()\n",
    "with open(f'{best_model_save_folder}/model_config.json', 'w') as f:\n",
    "    json.dump(model_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute the model on the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "x = []\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_true.append(labels.detach().clone())\n",
    "        y_pred.append(outputs.detach().clone())\n",
    "        x.append(inputs.detach().clone())\n",
    "y_true = torch.cat(y_true, dim=0).cpu()\n",
    "y_pred = torch.cat(y_pred, dim=0).cpu()\n",
    "X_test = torch.cat(x, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cm = torch.sigmoid(y_pred) >best_thresholds\n",
    "#calculate the confusion matrices on the test set\n",
    "confusion_matrices = multilabel_confusion_matrix(y_true, y_pred_cm)\n",
    "flipped_confusion_matrices = np.flip(confusion_matrices, axis=-1)\n",
    "flipped_confusion_matrices = np.flip(flipped_confusion_matrices, axis=-2)\n",
    "#plot the confusion matrices, one for each class, 3 per row\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(labels_to_encode) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 30))\n",
    "axes = axes.flatten()\n",
    "for i, ax in enumerate(axes):\n",
    "    #use the confusion matrix to plot the confusion matrix\n",
    "    cm = confusion_matrices[i]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=flipped_confusion_matrices[i], display_labels=['True', 'False'])\n",
    "    #turn off color bar\n",
    "\n",
    "    disp.plot(ax=ax, colorbar=False, cmap='bone')\n",
    "    title = labels_to_encode[i]\n",
    "    #split the title into multiple lines if it is too long\n",
    "    if title.count(' ') > 2:\n",
    "        #split every 3rd space\n",
    "        title = title.split(' ')\n",
    "        title = [' '.join(title[i:i+3]) for i in range(0, len(title), 3)]\n",
    "        title = '\\n'.join(title)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "#save the figure\n",
    "plt.savefig(f'{evaluations_save_folder}/confusion_matrices.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the same confusion matrices with the row normalised values\n",
    "cm_normalizer = confusion_matrices.sum(axis=2)#[:,np.newaxis]\n",
    "#add a new dimension to the normalizer\n",
    "cm_normalizer = cm_normalizer[:,np.newaxis]\n",
    "#add a copy of the normalizer to the normalizer\n",
    "\n",
    "#cm_normalizer = np.concatenate([cm_normalizer, cm_normalizer], axis=0)\n",
    "reversed_normalizer = np.flip(cm_normalizer, axis=-1)\n",
    "cm_normalizer=np.concatenate([cm_normalizer, reversed_normalizer], axis=1)\n",
    "#flip the normalizer\n",
    "cm_normalizer[:,:,1] = cm_normalizer[:,0,:]\n",
    "normalized_cms = confusion_matrices / cm_normalizer\n",
    "flipped_normalised_cms = np.flip(np.flip(normalized_cms, axis=-1), axis=-2)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 30))\n",
    "axes = axes.flatten()\n",
    "for i, ax in enumerate(axes):\n",
    "    #use the confusion matrix to plot the confusion matrix\n",
    "    cm = confusion_matrices[i]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=flipped_normalised_cms[i], display_labels=['True', 'False'])\n",
    "    #turn off color bar\n",
    "\n",
    "    disp.plot(ax=ax, colorbar=False, cmap='bone')\n",
    "    title = labels_to_encode[i]\n",
    "    #split the title into multiple lines if it is too long\n",
    "    if title.count(' ') > 2:\n",
    "        #split every 3rd space\n",
    "        title = title.split(' ')\n",
    "        title = [' '.join(title[i:i+3]) for i in range(0, len(title), 3)]\n",
    "        title = '\\n'.join(title)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    #save the figure\n",
    "plt.savefig(f'{evaluations_save_folder}/confusion_matrices_normalized.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show how many tp, fp, tn, fn there are in total\n",
    "tp = confusion_matrices[:, 1, 1]\n",
    "fp = confusion_matrices[:, 0, 1]\n",
    "tn = confusion_matrices[:, 0, 0]\n",
    "fn = confusion_matrices[:, 1, 0]\n",
    "print(f'TP: {tp.sum()}')\n",
    "print(f'FP: {fp.sum()}')\n",
    "print(f'TN: {tn.sum()}')\n",
    "print(f'FN: {fn.sum()}')\n",
    "print('-----------------------------------')\n",
    "best_thresholds = np.array(best_thresholds)\n",
    "#calculate the precision, recall, accuracy and f1 score\n",
    "precision_macro = multi_label_precision(y_true, y_pred, threshold=best_thresholds, averaging_type='macro')\n",
    "recall_macro = multi_label_recal(y_true, y_pred, threshold=best_thresholds, averaging_type='macro')\n",
    "accuracy_macro = multi_label_accuracy(y_true, y_pred, threshold=best_thresholds, averaging_type='macro')\n",
    "f1_macro = multi_label_f_beta(y_true, y_pred, beta=1.0, averaging_type='macro', threshold=best_thresholds)\n",
    "#do the same for the micro scores\n",
    "precision_micro = multi_label_precision(y_true, y_pred, threshold=best_thresholds, averaging_type='micro')\n",
    "recall_micro = multi_label_recal(y_true, y_pred, threshold=best_thresholds, averaging_type='micro')\n",
    "accuracy_micro = multi_label_accuracy(y_true, y_pred, threshold=best_thresholds, averaging_type='micro')\n",
    "f1_micro = multi_label_f_beta(y_true, y_pred, beta=1.0, averaging_type='micro', threshold=best_thresholds)\n",
    "\n",
    "#save the evaluation metrics in a dictionary\n",
    "evaluation_metrics = {\n",
    "    'precision_macro': precision_macro,\n",
    "    'recall_macro': recall_macro,\n",
    "    'accuracy_macro': accuracy_macro,\n",
    "    'f1_macro': f1_macro,\n",
    "    'precision_micro': precision_micro,\n",
    "    'recall_micro': recall_micro,\n",
    "    'accuracy_micro': accuracy_micro,\n",
    "    'f1_micro': f1_micro\n",
    "}\n",
    "#save the evaluation metrics\n",
    "with open(f'{evaluations_save_folder}/evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(evaluation_metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the metrics for every test dataset\n",
    "#best_thresholds = best_thresholds.cpu().numpy()\n",
    "score_dict = {'data_source' : [], 'f1_micro' : [], 'f1_macro': [], 'precision_micro': [], 'precision_macro': [], 'recall_micro': [], 'recall_macro': [], 'accuracy_micro': [], 'accuracy_macro': []}\n",
    "for test_dataset, matching_package in zip(test_datasets, test_packages):\n",
    "    y_true_single = []\n",
    "    y_pred_single = []\n",
    "    x_single = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    single_test_loader = DataLoader(test_dataset, batch_size=164, num_workers=num_workers)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in single_test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            y_true_single.append(labels.detach().clone())\n",
    "            y_pred_single.append(outputs.detach().clone())\n",
    "            x_single.append(inputs.detach().clone())\n",
    "    y_true_single = torch.cat(y_true_single, dim=0).cpu()\n",
    "    y_pred_single = torch.cat(y_pred_single, dim=0).cpu()\n",
    "    x_single = torch.cat(x_single, dim=0).cpu()\n",
    "    y_pred_single_cm = torch.sigmoid(y_pred_single) > torch.Tensor(best_thresholds)\n",
    "    #calculate precision, recall, accuracy and f1 score\n",
    "    precision = multi_label_precision(y_true_single, y_pred_single, threshold=best_thresholds, averaging_type='macro')\n",
    "    recall = multi_label_recal(y_true_single, y_pred_single, threshold=best_thresholds, averaging_type='macro')\n",
    "    accuracy = multi_label_accuracy(y_true_single, y_pred_single, threshold=best_thresholds, averaging_type='macro')\n",
    "    f1 = multi_label_f_beta(y_true_single, y_pred_single, beta=1.0, averaging_type='macro', threshold=best_thresholds)\n",
    "    #and micro scores\n",
    "    precision_micro = multi_label_precision(y_true_single, y_pred_single, threshold=best_thresholds, averaging_type='micro')\n",
    "    recall_micro = multi_label_recal(y_true_single, y_pred_single, threshold=best_thresholds, averaging_type='micro')\n",
    "    accuracy_micro = multi_label_accuracy(y_true_single, y_pred_single, threshold=best_thresholds, averaging_type='micro')\n",
    "    f1_micro = multi_label_f_beta(y_true_single, y_pred_single, beta=1.0, averaging_type='micro', threshold=best_thresholds)\n",
    "    #save the scores\n",
    "    score_dict['data_source'].append(matching_package.data_source_name)\n",
    "    score_dict['f1_micro'].append(f1_micro)\n",
    "    score_dict['f1_macro'].append(f1)\n",
    "    score_dict['precision_micro'].append(precision_micro)\n",
    "    score_dict['precision_macro'].append(precision)\n",
    "    score_dict['recall_micro'].append(recall_micro)\n",
    "    score_dict['recall_macro'].append(recall)\n",
    "    score_dict['accuracy_micro'].append(accuracy_micro)\n",
    "    score_dict['accuracy_macro'].append(accuracy)\n",
    "\n",
    "#save the scores to a json file\n",
    "with open(f'{evaluations_save_folder}/break_down_scores.json', 'w') as f:\n",
    "    json.dump(score_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pandas dataframe from the score dict\n",
    "score_df = pd.DataFrame(score_dict)\n",
    "#group the scores by data source\n",
    "grouped_scores = score_df.groupby('data_source').mean()\n",
    "#plot the grouped scores\n",
    "grouped_scores.plot(kind='bar', figsize=(20, 10))\n",
    "plt.savefig(f'{evaluations_save_folder}/grouped_scores.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ird_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
